# 数据爬虫

## 相关内容

打开 [丁香园官方网址](https://ncov.dxy.cn/ncovh5/view/pneumonia)，如下图：

<center>
<img src="https://www.educoder.net/api/attachments/1741769" width="85%" height="85%">
</center>



在上面我们可以看到很多疫情相关的数据，包括地区风险数据、国内数据、全球数据和新冠疫苗数据。

![,](https://www.educoder.net/api/attachments/1741783)


现在开始来对国内数据进行爬虫，找到我们需要的数据内容，浏览器按下 F12 按钮进入网页检查。

![,](https://www.educoder.net/api/attachments/1741806)

点开 `id` 为 `getAreaStat`的 `script` 标签内容，发现里面有个 Json 数据包含着全国各省各市今天的疫情数据。

![,](https://www.educoder.net/api/attachments/1741811)

## 爬取我国今天的疫情数据
现在对我国今天的疫情数据进行爬取，代码如下：

1.访问网址，获取网页内容



```python
#导入 requests 库
import requests

# 通过 requests 库get访问https://ncov.dxy.cn/ncovh5/view/pneumonia网址，获取网页内容
htmlContent=requests.get("https://ncov.dxy.cn/ncovh5/view/pneumonia")

# 输出网页内容(输出内容太多，会导致jupyter卡死，可以在本地尝试，这里暂不开放)
# print(htmlContent.text)
```

2.获取标签内容


获取到了网页内容，现在需要获取 `id` 为 `getAreaStat`的 `script` 标签内容，我们采用 BeautifulSoup 方法来获取 `script` 标签的位置解析页面中的指定内容，获取我国今天的疫情数据。
取到的网页文字内容在编码上存在一定的 `trick` ，简单来说就是 `unicode` 形式的 `gbk` 编码内容，形如： `u'\xd6\xb0\xce\xbb\xc3\xe8\xca\xf6'`

而事实上，这个字符串实际所要表达的 `gbk` 编码内容为
`'\xd6\xb0\xce\xbb\xc3\xe8\xca\xf6'`，对应的汉字字符为“职位描述”

先将 `unicode` 字符串编码为 `latin1` 字符串，编码后保留了等价的字节流数据。
而此时在这个问题中，这一字节流数据又恰恰对应了 `gbk` 编码，因此对其进行解码即可还原最初的 `unicode` 字符。
不过值得注意的是，需要确定的是形如`\xd6\xb0` 究竟是 `utf8` 编码还是类似 `gbk` 类型的其他编码，
这一点对于最终正确还原 `unicode` 字符同样重要。

综上所述，对拿到的 `content` 执行以下操作即可：
`content.encode("latin1").decode("utf-8")`



```python
from bs4 import BeautifulSoup
#读取网页，
soup = BeautifulSoup(htmlContent.text.encode("latin1").decode('utf-8'),'lxml')
#查找script标签id为getAreaStat
tag=str(soup.find('script',id='getAreaStat'))

#输出内容
#print(tag)
```

3.获取标签内容的json数据并使用json库解析数据。

标签内容已经拿到了，需要将标签内的json数据单独拿出并解析。可以使用正则方式来获取，也可以使用替换的方式来将其他不需要的字符串给剔除。


```python
# 正则处理
import re

today_data_json=re.findall('window.getAreaStat = (.*?)}catch\(e\)',tag, re.S)[0]

# 替换处理
#today_data_json=tag.replace('<script id="getAreaStat">try { window.getAreaStat = ','').replace('}catch(e){}</script>','')


# json解析
import json

jsonContext = json.loads(today_data_json)

#print(jsonContext)
```

4. 遍历数据，获取各字段内容。

解析完后，我们进一步来查看 json 中 key 和 value 的有一些什么含义。

可以使用 [Json 在线解析工具](https://www.sojson.com/)来将 today_data_json 进行排版。


![,](https://www.educoder.net//api/attachments/1744298)


各字段分析如下：

|name|type|describe|
| ------ | :-------: | :-------: |
|provinceName|string|省份名称|
|provinceShortName|string|省份短名|
|currentConfirmedCount|int|当前确诊人数|
|confirmedCount|int|累计确诊人数|
|suspectedCount|int|疑似病例人数|
|curedCount|int|累计确诊人数治愈人数|
|deadCount|int|死亡人数|
|comment|string|备注|
|locationId|int|省份位置id|
|statisticsData|string|历史统计数据链接|
|highDangerCount|int|高风险地区人数|
|midDangerCount|int|中风险地区人数|
|detectOrgCount|int|检测人数|
|vaccinationOrgCount|int|接种疫苗数量|
|cities|list|下属城市列表|
|dangerAreas|list|危险区域|


cities（城市列表）数据字段分析如下：

|name|type|describe|
| ------ | :-------: | :-------: |
|cityName|string|城市名称|
|currentConfirmedCount| int|当前确诊人数|
|confirmedCount| int|累计确诊人数|
|suspectedCount|int |疑似病例人数|
|curedCount| int|累计确诊人数治愈人数|
|deadCount| int|死亡人数|
|highDangerCount|int |高风险地区人数|
|midDangerCount| int|中风险地区人数|
|locationId| |城市位置id|
|currentConfirmedCountStr|string|当前确诊人数状态(+代表新增；-代表减少)|



dangerAreas（危险地区）数据字段分析如下：

|name|type|describe|
| ------ | :-------: | :-------: |
|cityName|string|城市名 |
|areaName|string|地区名|
|dangerLevel|int|危险等级|





我们已经将 today_data_json 解析为 json 格式了，现在只需要将数据遍历，再将里面数据变为json格式并数据获取字段值，代码如下：



```python
#遍历json，获取省当天疫情数据
#for a in jsonContext:
    #print(a)
```


```python
#创建对应列表
#放入省疫情数据
province_list=[]
#放入市/区疫情数据
city_list=[]
#放入危险区域数据
dangerAreas_list=[]

for province_data in jsonContext:

    provinceName = str(province_data['provinceName'])
    provinceShortName = str(province_data['provinceShortName'])
    currentConfirmedCount = int(province_data['currentConfirmedCount'])
    confirmedCount = int(province_data['confirmedCount'])
    suspectedCount = int(province_data['suspectedCount'])
    curedCount = int(province_data['curedCount'])
    deadCount = int(province_data['deadCount'])
    comment = str(province_data['comment'])
    locationId = int(province_data['locationId'])
    statisticsData = str(province_data['statisticsData'])
    highDangerCount = int(province_data['highDangerCount'])
    midDangerCount = int(province_data['midDangerCount'])
    detectOrgCount = int(province_data['detectOrgCount'])
    vaccinationOrgCount = int(province_data['vaccinationOrgCount'])
    
    #城市疫情数据
    cities_json = province_data['cities']

    for cities_data in cities_json:
        cityName=cities_data['cityName']
        city_currentConfirmedCount=cities_data['currentConfirmedCount']
        city_confirmedCount=cities_data['confirmedCount']
        city_suspectedCount=cities_data['suspectedCount']
        city_curedCount=cities_data['curedCount']
        city_deadCount=cities_data['deadCount']
        city_highDangerCount=cities_data['highDangerCount']
        city_midDangerCount=cities_data['midDangerCount']
        city_locationId=cities_data['locationId']
        city_currentConfirmedCountStr=cities_data['currentConfirmedCountStr']
        #将获取到的数据放入对应列表中（添加省份名）
        city_list.append([provinceName, cityName,city_locationId,city_currentConfirmedCount,city_confirmedCount,city_suspectedCount,
        city_curedCount,city_deadCount,city_highDangerCount,city_midDangerCount,city_currentConfirmedCountStr])


    #危险区域疫情数据
    dangerAreas_json = province_data['dangerAreas']

    for dangerAreas_data in dangerAreas_json:
        cityName=dangerAreas_data['cityName']
        areaName=dangerAreas_data['areaName']
        dangerLevel=dangerAreas_data['dangerLevel']
        #将获取到的数据放入对应列表中（添加省份名）
        dangerAreas_list.append([provinceName,cityName,areaName,dangerLevel])

    
    province_list.append([provinceName,provinceShortName,locationId,currentConfirmedCount,confirmedCount,suspectedCount,curedCount,deadCount,comment,statisticsData,highDangerCount,midDangerCount,detectOrgCount,vaccinationOrgCount])

    
#输出省份数据
#print(province_list)
#输出城市数据
#print(city_list)
#输出危险区域数据
#print(dangerAreas_list)
    
```

5.保存数据为 csv 文件

获得三类数据后，我们分别将每一类数据保存为一个数据，命名为：当天时间+类型数据。

代码如下：


```python
#导入pandas,time包
import pandas as pd

import time

#获得当天时间，格式为“YYYY-MM-DD”
today_date=time.strftime("%Y-%m-%d", time.localtime())
#设置列名
data1 =['省份名称','省份短名','省份位置id','当前确诊人数','累计确诊人数','疑似病例人数','累计确诊人数治愈人数','死亡人数','备注','历史统计数据链接','高风险地区人数','中风险地区人数','检测人数','接种疫苗数量']
#创建 Dataframe 将列名和数据列表放入表中
frame1=pd.DataFrame(province_list,columns=data1)
#保存数据至/root 目录下
frame1.to_csv('/root/'+today_date+"_China_Province_COVID19_Data.csv",encoding='utf-8',index=False)

#设置列名
data2=['省份名称','城市名称','城市位置id','当前确诊人数','累计确诊人数','疑似病例人数','累计确诊人数治愈人数','死亡人数','高风险地区人数','中风险地区人数','当前确诊人数状态']
#创建 Dataframe 将列名和数据列表放入表中
frame2=pd.DataFrame(city_list,columns=data2)
#保存数据至/root 目录下
frame2.to_csv("/root/"+today_date+"_China_City_COVID19_Data.csv",encoding='utf-8',index=False)

#设置列名
data3=['省份名称','城市名称','地区名','危险等级']
#创建 Dataframe 将列名和数据列表放入表中
frame3=pd.DataFrame(dangerAreas_list,columns=data3)
#保存数据至/root 目录下
frame3.to_csv("/root/"+today_date+"_China_DangerAreas_COVID19_Data.csv",encoding='utf-8',index=False)
```

## 爬取中国省内历史数据

上面 jsonContext 中的 statisticsData 中字段为历史统计数据链接，我们访问进去


```python
#访问香港历史数据链接
htmlContent=requests.get('https://file1.dxycdn.com/2020/0223/331/3398299755968040033-135.json')
#输出内容
#print(htmlContent.text)
```

1.解析数据为json格式，获取数据内容

其中数据内容字段分析如下：

|name|type|describe|
| ------ | :-------: | :-------: |
|confirmedCount|int | 确诊人数总数|
|confirmedIncr | int| 新增确诊人数|
|curedCount |int |治愈人数总数|
|curedIncr | int|新增治愈人数|
|currentConfirmedCount |int |现在确诊人数|
|currentConfirmedIncr |int |现在新增确诊人数|
|dateId |int |日期|
|deadCount | int|死亡人数|
|deadIncr |int |新增死亡人数|
|highDangerCount |int| 高风险地区|
|midDangerCount |int |中风险地区|
|suspectedCount |int |疑似人数|
|suspectedCountIncr |int |新增疑似人数|


```python
province_history_data=json.loads(htmlContent.text)['data']
#遍历json数据
list_ = []
for province in province_history_data:

    confirmedCount = int(province['confirmedCount'])
    confirmedIncr = int(province['confirmedIncr'])
    curedCount = int(province['curedCount'])
    curedIncr = int(province['curedIncr'])
    currentConfirmedCount = int(province['currentConfirmedCount'])
    currentConfirmedIncr = int(province['currentConfirmedIncr'])
    dateId = int(province['dateId'])
    deadCount = int(province['deadCount'])
    deadIncr = int(province['deadIncr'])
    highDangerCount = int(province['highDangerCount'])
    midDangerCount = int(province['midDangerCount'])
    suspectedCount = int(province['suspectedCount'])
    suspectedCountIncr = int(province['suspectedCountIncr'])

    list_.append(
        [dateId, confirmedCount, confirmedIncr, curedCount, curedIncr, curedCount, currentConfirmedCount, deadCount,
         deadIncr,highDangerCount,midDangerCount,suspectedCount,suspectedCountIncr])
#输出内容
#print(list_)
    
```

2.爬取所有省份历史数据

刚刚只是测试爬取单个省份的历史数据，现在开始爬取所有省份的历史数据。原理也是一样，只是在外面多嵌套了一层循环。


```python
#遍历json数据内容
for province_data in jsonContext:
    province = requests.get(province_data['statisticsData']).text
    data_json = json.loads(province)['data']

    list_ = []
    for data in data_json:
        confirmedCount = int(data['confirmedCount'])
        confirmedIncr = int(data['confirmedIncr'])
        curedCount = int(data['curedCount'])
        curedIncr = int(data['curedIncr'])
        currentConfirmedCount = int(data['currentConfirmedCount'])
        currentConfirmedIncr = int(data['currentConfirmedIncr'])
        dateId = int(data['dateId'])
        deadCount = int(data['deadCount'])
        deadIncr = int(data['deadIncr'])
        highDangerCount = int(data['highDangerCount'])
        midDangerCount = int(data['midDangerCount'])
        suspectedCount = int(data['suspectedCount'])
        suspectedCountIncr = int(data['suspectedCountIncr'])
        list_.append(
            [dateId, confirmedCount, confirmedIncr, curedCount, curedIncr, curedCount, currentConfirmedCount, deadCount,
             deadIncr, highDangerCount, midDangerCount, suspectedCount, suspectedCountIncr])

    data1 = ['日期', '确诊人数总数', '新增确诊人数', '治愈人数总数', '新增治愈人数', '现在确诊人数', '现在新增确诊人数', '死亡人数', '新增死亡人数','高风险地区','中风险地区','疑似人数','新增疑似人数']

    frame = pd.DataFrame(list_, columns=data1)
    #因为中文可能保存文件名报错，所以这边保存为不带中文的文件名
    #result = "../result/" + str(province_data['provinceName']) + "的疫情历史数据.csv"

    result = "/root/" + str(province_data['locationId']) + "_data.csv"

    frame.to_csv(result, encoding='utf-8', index=False)
```

数据保存完成，结果文件夹如下：
![,](https://www.educoder.net/api/attachments/1746205)
文件内容如下（其一）：
![,](https://www.educoder.net/api/attachments/1746208)


## 爬取获取全球总数据疫情历史数据


以上爬取的是中国当天的疫情数据和省内历史数据，现在通过另一组数据来获取全球总疫情统计数据和各国历史数据。


另一组也是通过script标签传递过来的，如下图：
![,](https://www.educoder.net//api/attachments/1748290)

我们就可以使用同样的方法进行爬取。


```python
#3.查找script标签id为getAreaStat
tag=str(soup.find('script',id='getListByCountryTypeService2true'))

data_json=re.findall('window.getListByCountryTypeService2true = (.*?)}catch\(e\)',tag, re.S)

# json解析
jsonContext = json.loads(data_json[0])
#print(jsonContext)
```

1.字段分析，去除无用数据


拿出一组 json 数据进行分析字段
```
{
	"id": 8451530,
	"createTime": 1619570396000,
	"modifyTime": 1619570396000,
	"tags": "",
	"countryType": 2,
	"continents": "北美洲",
	"provinceId": "8",
	"provinceName": "美国",
	"provinceShortName": "",
	"cityName": "",
	"currentConfirmedCount": 6078180,
	"confirmedCount": 32168304,
	"confirmedCountRank": 1,
	"suspectedCount": 0,
	"curedCount": 25516808,
	"deadCount": 573316,
	"deadCountRank": 1,
	"deadRate": "1.78",
	"deadRateRank": 84,
	"comment": "",
	"sort": 0,
	"operator": "xialingxiao",
	"locationId": 971002,
	"countryShortCode": "USA",
	"countryFullName": "United States of America",
	"statisticsData": "https://file1.dxycdn.com/2020/0315/553/3402160512808052518-135.json",
	"incrVo": {
		"currentConfirmedIncr": -47094,
		"confirmedIncr": 90999,
		"curedIncr": 136977,
		"deadIncr": 1116
	},
	"showRank": true,
	"yesterdayConfirmedCount": 2147383647,
	"yesterdayLocalConfirmedCount": 2147383647,
	"yesterdayOtherConfirmedCount": 2147383647,
	"highDanger": "",
	"midDanger": "",
	"highInDesc": "",
	"lowInDesc": "",
	"outDesc": ""
}
```


|name|type|describe|
| :------ | :-------: | :-------: |
|id|int | ID|
|createTime|int | 创建时间|
|modifyTime| int| 修改时间|
|tags|string |标签|
|countryType| int|国家类型|
|continents| string|大洲名称|
|provinceId|string |国家ID|
|provinceName| string| 国家名称|
|provinceShortName|string | 国家简称|
|cityName|string | 城市名|
|currentConfirmedCount| int|现在确诊人数|
|confirmedCount|int |确诊人数总数|
|confirmedCountRank| int| 确诊人数数量排名|
|suspectedCount| int| 疑似人数|
|curedCount|int | 累计确诊人数治愈人数
|deadCount| int| 死亡人数|
|deadCountRank|int | 死亡人数数量排名|
|deadRate|string | 死亡比率|
|deadRateRank| int| 死亡比率排名|
|comment|string |  备注|
|sort| int|分类|
|operator| string| 操作员|
|locationId|int |位置id |
|countryShortCode| string| 国家短名|
|countryFullName|string | 国家全称|
|statisticsData|string |  历史统计数据链接|
|incrVo| dict | 当天新增数据|
|showRank|boolean | 显示排名|
|yesterdayConfirmedCount| int|  昨天确诊数量|
|yesterdayLocalConfirmedCount|int |昨天本地确诊数量|
|yesterdayOtherConfirmedCount|int | 昨天其他确诊数量|
|highDanger| string| 高风险|
|midDanger| string|  中风险|
|highInDesc|string | 描述高|
|lowInDesc| string|  描述低|
|outDesc| string| 输出描述|


其中 incrVo 字段中存在一个字典：

|name|type|describe|
| :------ | :-------: | :-------: |
|currentConfirmedIncr|int|较昨日确诊人数|
|confirmedIncr|int|确诊新增|
|curedIncr|int|治愈新增|
|deadIncr|int|死亡新增|


这里一共有36个字段，我们采集的数据并不需要那么多的字段，所以我们针对关键有用字段进行爬取分别是：

|name|type|describe|
| :------ | :-------: | :-------: |
|continents| string|大洲名称|
|provinceName| string| 国家名称|
|currentConfirmedCount| int|现在确诊人数|
|confirmedCount|int |确诊人数总数|
|confirmedCountRank| int| 确诊人数数量排名|
|curedCount|int | 累计确诊人数治愈人数
|deadCount| int| 死亡人数|
|deadCountRank|int | 死亡人数数量排名|
|deadRate|string | 死亡比率|
|deadRateRank| int| 死亡比率排名|
|currentConfirmedIncr|int|较昨日确诊人数|
|confirmedIncr|int|确诊新增|
|curedIncr|int|治愈新增|
|deadIncr|int|死亡新增|

并通过 statisticsData 字段获取所有国家的历史数据


代码如下：


```python
globals_list=[]
country_list=[]
for globals_data in jsonContext:
    continents=globals_data['continents']
    provinceName=globals_data['provinceName']
    currentConfirmedCount=globals_data['currentConfirmedCount']
    confirmedCount=globals_data['confirmedCount']
    #如果showRank开关未打开则输出“”
    if globals_data['showRank']==True:
        confirmedCountRank=globals_data['confirmedCountRank']
        deadCountRank=globals_data['deadCountRank']
        deadRateRank = globals_data['deadRateRank']
    else:
        confirmedCountRank = ""
        deadCountRank=""
        deadRateRank=""


    curedCount=globals_data['curedCount']
    deadCount=globals_data['deadCount']
    deadRate=globals_data['deadRate']
    statisticsData=globals_data['statisticsData']
    currentConfirmedIncr=globals_data['incrVo']['currentConfirmedIncr']
    confirmedIncr=globals_data['incrVo']['confirmedIncr']
    curedIncr=globals_data['incrVo']['curedIncr']
    deadIncr=globals_data['incrVo']['deadIncr']
    globals_list.append([continents,provinceName,currentConfirmedCount,confirmedCount,confirmedCountRank,
                  curedCount,deadCount,deadCountRank,deadRate,deadRateRank,currentConfirmedIncr,confirmedIncr,curedIncr,deadIncr])

    country_data = requests.get(globals_data['statisticsData']).text
    country_json=json.loads(country_data)['data']
    for countrys in country_json:
        confirmedCount = int(countrys['confirmedCount'])
        confirmedIncr = int(countrys['confirmedIncr'])
        curedCount = int(countrys['curedCount'])
        curedIncr = int(countrys['curedIncr'])
        currentConfirmedCount = int(countrys['currentConfirmedCount'])
        currentConfirmedIncr = int(countrys['currentConfirmedIncr'])
        dateId = int(countrys['dateId'])
        deadCount = int(countrys['deadCount'])
        deadIncr = int(countrys['deadIncr'])
        highDangerCount = int(countrys['highDangerCount'])
        midDangerCount = int(countrys['midDangerCount'])
        suspectedCount = int(countrys['suspectedCount'])
        suspectedCountIncr = int(countrys['suspectedCountIncr'])
        country_list.append(
            [dateId,continents,provinceName,confirmedCount, confirmedIncr, curedCount, curedIncr, curedCount, currentConfirmedCount, deadCount,
             deadIncr, highDangerCount, midDangerCount, suspectedCount, suspectedCountIncr])

#保存csv
data1 = ['大洲名', '国家名', '现在确诊人数', '确诊人数总数','确诊人数数量排名', '累计确诊人数治愈人数', '死亡人数', '死亡人数数量排名', '死亡比率', '死亡比率排名',
              '较昨日确诊人数', '确诊新增', '治愈新增','死亡新增']
frame1 = pd.DataFrame(globals_list, columns=data1)
frame1.to_csv("/root/Global_total_data.csv", encoding='utf-8', index=False)



data2 = ['日期','大洲名','国家名', '确诊人数总数', '新增确诊人数', 
         '治愈人数总数', '新增治愈人数', '现在确诊人数', '现在新增确诊人数', '死亡人数', '新增死亡人数','高风险地区','中风险地区','疑似人数','新增疑似人数']
frame2 = pd.DataFrame(country_list, columns=data2)
frame2.to_csv("/root/country_history_data.csv", encoding='utf-8', index=False)        
        
```

运行时间可能较长，请耐心等待一会。。。

成功发现数据已经爬取成功，内容如下：
![,](https://www.educoder.net//api/attachments/1748999)
![,](https://www.educoder.net//api/attachments/1749000)

至此，疫情爬虫实战结束
